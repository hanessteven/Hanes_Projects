{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Profitable Apps \n",
    "For this project we will assume the role of a data analyst for a mobile app development company.\n",
    "**Our goal is to build a free app that generates revenue by new ad views.**  This implies that the more users who\n",
    "see the add/install the app, the more revenue we will generate.\n",
    "\n",
    "\n",
    "This project will involve analyzing available app store data to find predictors of an app being likely to attract\n",
    " new users.  Assuming we find good indicators for number of installs, we will attempt to build a predictive model that \n",
    " given early ratings could tell us whether the app is likely to have a lot of installs or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Opening and Exploring the Data\n",
    "*We will be working with a dataset from 2018 as it was readily available.  It consists or approximately 10k+ apps from \n",
    "the Google App markets.  This dataset is available via Kaggle at the below link*\n",
    "- [Google Play Store Data](https://www.kaggle.com/lava18/google-play-store-apps)\n",
    "\n",
    "\n",
    "Using this sample of the true dataset should suffice at achieving our goal without making the investment to collect \n",
    "a larger dataset.  It is important to note the assumption that the marketplace hasn't changed in the time period in \n",
    "following the dataset.  In a production environment this would only serve as a proof of concept and an opportunity\n",
    "to gain some domain knowledge on feature importance.\n",
    "\n",
    "**Lets start by opening the data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTS #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import jenkspy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "google_raw = pd.read_csv(r'RawData/googleplaystore.csv')  # Read the csv file into a pandas Dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One thing to always be conscious of is where your data is coming from.  In our case, the Google Dataset from Kaggle has \n",
    "a discussion section and one top discussion involves an inaccuracy in our data.  Apparently the 'Life Made Wi-Fi \n",
    "Touchscreen Photo Frame' app has invalid data.  In this case, since it is such a small sample of our data, we will just\n",
    "remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([10472], dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-880a83b4813d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgoogle_raw\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m10472\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# After verifying we have the correct index, drop it using .drop\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgoogle_raw\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgoogle_raw\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m10472\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hanes\\ideaprojects\\venv\\dataquest64\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   1766\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1767\u001B[0m             \u001B[0mmaybe_callable\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_if_callable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1768\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_axis\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmaybe_callable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1769\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1770\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_is_scalar_access\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTuple\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hanes\\ideaprojects\\venv\\dataquest64\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_getitem_axis\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1952\u001B[0m                     \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Cannot index with multidimensional key\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1953\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1954\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_iterable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1955\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1956\u001B[0m             \u001B[1;31m# nested tuple slicing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hanes\\ideaprojects\\venv\\dataquest64\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_getitem_iterable\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1593\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1594\u001B[0m             \u001B[1;31m# A collection of keys\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1595\u001B[1;33m             \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_listlike_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mraise_missing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1596\u001B[0m             return self.obj._reindex_with_indexers(\n\u001B[0;32m   1597\u001B[0m                 \u001B[1;33m{\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mallow_dups\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hanes\\ideaprojects\\venv\\dataquest64\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_get_listlike_indexer\u001B[1;34m(self, key, axis, raise_missing)\u001B[0m\n\u001B[0;32m   1551\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1552\u001B[0m         self._validate_read_indexer(\n\u001B[1;32m-> 1553\u001B[1;33m             \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mo\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_axis_number\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mraise_missing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mraise_missing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1554\u001B[0m         )\n\u001B[0;32m   1555\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hanes\\ideaprojects\\venv\\dataquest64\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_validate_read_indexer\u001B[1;34m(self, key, indexer, axis, raise_missing)\u001B[0m\n\u001B[0;32m   1638\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mmissing\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1639\u001B[0m                 \u001B[0maxis_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_axis_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1640\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1641\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1642\u001B[0m             \u001B[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: \"None of [Int64Index([10472], dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "print(google_raw.loc[[10472]])  # After verifying we have the correct index, drop it using .drop\n",
    "df = google_raw.drop(google_raw.index[10472])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cleaning the Data and Creating Features\n",
    "\n",
    "Now that we have dropped the advised row of incorrect data, we should spend some time ourselves checking against common\n",
    "issues associated with unknown datasets.  The first area to check is for duplicate entries in our dataset. Since our\n",
    "dataset is defined at the app level, we can check this by checking for duplicate application names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dups = df[df.duplicated(['App'])].App  # Create a DF and then filter to a Series of all Duplicated App Names\n",
    "sorted(set(dups))[:10]  # Convert to a set to remove duplicates, sort them and print out the first 10 for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see, there are quite a few duplicate entries in our dataset.  Now that we see a preview of some examples \n",
    "we can take a look at their raw data to see if and where their differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[df.App == '10 Best Foods for You']  # Lookup the App by Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "df[df.App == 'Instagram']  # Lookup the App by Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Through some quick investigation it is clear that the area in which there are often differences in the duplicated apps \n",
    "is in the total number of reviews given to that app.  There are a few options here, but I believe the most efficient use\n",
    " of our time is to go with the App with the highest review count.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=['Reviews'], inplace=True, ascending=False)  # Sort rows by highest reviews to lowest\n",
    "# Remove any duplicate App names, keeping the first occurrence\n",
    "df.drop_duplicates(subset='App', keep='first', inplace=True)\n",
    "\n",
    "# Check for dups again to confirm they were removed\n",
    "dups = df[df.duplicated(['App'])].App\n",
    "print(sorted(set(dups))[:10])\n",
    "\n",
    "df[df.App == 'Instagram']  # Verify we removed all dups keeping the highest review count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can now see we have removed all duplicate App names from our Google dataset.  We've also displayed the results \n",
    "for instagram so that we can check we removed duplicates correctly.\n",
    "\n",
    "The next step seems to be to start analyzing the data from our slightly cleaned set.  **However, if we keep the goal \n",
    "of this project in mind, we will remember that we do not care about paid or Non-English apps.**  Our goal is to make a \n",
    "free, english friendly app.  it is important to remove these other apps from our dataset early on so that we do not\n",
    "consider their values as valid while cleaning and eventually feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def is_english(row: pd.DataFrame, met: str, flag: int) -> bool:\n",
    "    \"\"\"\n",
    "    :param row: a row from a pandas dataFrame\n",
    "    :param met: a column header to determine the column checking\n",
    "    :param flag: an int provided to serve as the threshold for how many non-ASCII characters are allowed\n",
    "    :return: a boolean that returns True if it is english\n",
    "    \n",
    "    This function uses python's built-in ord function to find out the corresponding number of each character.\n",
    "    English characters (including punc) are encoded as ASCII standard, meaning each character should fall within 0 and\n",
    "    127.  However there are instances where english apps use non ASCII characters (emojis, dashes, etc..)  Due to this \n",
    "    we will set a threshold for the number of non-ASCII characters the string can have.\n",
    "    \n",
    "    TODO: may be worth having the threshold provided systematically based on length of string, etc..\n",
    "    \"\"\"\n",
    "    non_ascii = 0  # Establish counter\n",
    "    for char in row[met]:  # For each character in the string\n",
    "        if ord(char) > 127:  # If non-english\n",
    "            non_ascii += 1  # add 1 to the counter\n",
    "    \n",
    "    if non_ascii > flag:  # Post Loop, if counter above threshold, return False\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "df = df[df.apply(is_english, axis=1, met='App', flag=3) == True]  # Apply is_enlgish function row by row on the App col\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Removing price is much simpler, as we have a categorical variable for Free Vs Paid in our dataset.  It is important in \n",
    "these situation to check your classifying variables.  So we should look to see if there are cases in which the App is \n",
    "labeled as paid, but appears to have no cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check to ensure there are not invalid cases of this classification variable\n",
    "invalid = df[(df.Price == '0') & (df.Type == 'Paid')]\n",
    "print(invalid)\n",
    "\n",
    "# Double Check that all we have remaining is \"Free\" apps\n",
    "df = df[df.Type != 'Paid']\n",
    "df.Type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make further data exploration easier we will start by exploring the columns and the data types contained\n",
    "in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_cat(series: pd.Series, empty_flag: bool) -> str:\n",
    "    \"\"\"\n",
    "    :param series: a pandas series of data (should be of one data type)\n",
    "    :param empty_flag: a boolean to call out an empty column (containing all NaNs)\n",
    "    :return: a string that is the 'category of the datatype'\n",
    "    \n",
    "    This function will take a given series and return a generalized category for the datatype.\n",
    "    This is necessary for readability as df.dtypes will return ungrouped types (i.e. float64, int64)\n",
    "    as well as return some options that do not provide much insight (i.e. object)\n",
    "    \"\"\"\n",
    "    if empty_flag:  # Catch Empty Columns\n",
    "        return 'Empty'\n",
    "    elif pd.api.types.is_numeric_dtype(series):\n",
    "        return 'Numerical'\n",
    "    elif pd.api.types.is_datetime64_dtype(series):\n",
    "        return 'Date'\n",
    "    elif pd.api.types.is_bool_dtype(series):\n",
    "        return 'Boolean'\n",
    "    elif pd.api.types.is_categorical_dtype(series):\n",
    "        return 'Categorical'\n",
    "    elif pd.api.types.is_string_dtype(series):\n",
    "        return 'Text'\n",
    "    else:  # Catches all others for review\n",
    "        return 'Unknown'\n",
    "\n",
    "\n",
    "def col_categories(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    :param data: a pandas Dataframe consisting of columns of data you want to explore their categories\n",
    "    :return: None\n",
    "    Prints the column name and data type of each column in the dataframe \n",
    "    \"\"\"\n",
    "    if data.dropna().empty:  # Catches Error\n",
    "        raise ValueError('DataFrame Provided Contains is either empty or only contains NaNs') \n",
    "        # sys.exit(1)\n",
    "    for col in data.columns:  # For Each Column\n",
    "        if data[col].dropna().empty:  # Catch Empty Columns\n",
    "            empty = True  # Set Flag\n",
    "        else:\n",
    "            empty = False  # Set Flag\n",
    "        print(col, ': ', get_cat(data[col], empty))  # Print and Call Column Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Call col_categories to get an understanding of our Datasets \n",
    "print('Google Data')\n",
    "col_categories(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have cleaned up our dataset a little, we can seem some areas where we may be suspicious of the data type \n",
    "returned to us.  For example, \"Category\" in the Google data could potentially be converted into a categorical variables. \n",
    "While making these conversions may not be the most crucial step we will do here today, it serves as a good step to \n",
    "ensure later on you can grab data by type. Even for metrics we may not end up using, it is reasonable to do some basic \n",
    "level of cleaning and converting so that this data can be exported and saved for future uses (which may need the other \n",
    "features)\n",
    "\n",
    "**The first step is to confirm our assumptions on data types by looking at samples of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the first few rows of our data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*The user should be looking at more than the first 5 rows of data to get a good understanding of the data, this \n",
    "amount is just used for readability*\n",
    "\n",
    "### As we expected, we have a few areas worth converting/cleaning:\n",
    "- **Category**\n",
    "    - From Text to Category\n",
    "- **Size** - Seems like there are 2 Options \n",
    "    - Try converting to a numerical and impute the \"Varies By Size\"\n",
    "    - Explore the distribution of this metric to see if we can create ranges and thus categories\n",
    "- **Installs**\n",
    "    - From Text to Categorical\n",
    "- **Type**\n",
    "    - From Text to Categorical\n",
    "- **Content Rating**\n",
    "    - From Text to Categorical\n",
    "- **Genre**\n",
    "    - Multiple Entry Text, split on ; - Will probably be worth to dummify the variable to turn them into multiple binary\n",
    "    variables\n",
    "- **Last Updated**\n",
    "    - From Text to Date\n",
    "\n",
    "*Current Version and Android Version Required are not something I will be working with as I think those categories are \n",
    "more complicated in their feature engineering, and we want to prioritize our time in the most effective way.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Simple Text to Categorical Conversion\n",
    "df['Category'] = pd.Categorical(df.Category)\n",
    "df['Installs'] = pd.Categorical(df.Installs)\n",
    "df['Type'] = pd.Categorical(df.Type)\n",
    "df['Content Rating'] = pd.Categorical(df['Content Rating'])\n",
    "df['Reviews'] = df['Reviews'].astype('int')\n",
    "\n",
    "#  Recall our Original Col DataType Function\n",
    "print('Google Data')\n",
    "col_categories(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have completed the one step conversion of data types, we can move onto the more complicated.  We will\n",
    "start by analyzing the Size Category.  I believe the best process will be to first analyze the data we have for google.\n",
    "\n",
    "By examining the data in the CSV file we can see they have a standard data structure format. We can use regular \n",
    "expressions to analyze this more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def patt_out(row: pd.DataFrame, met:str, patt:str, result:list):\n",
    "    \"\"\"\n",
    "    :param row: row supplied by the apply function\n",
    "    :param met: the column of interest from the row\n",
    "    :param patt: Regex pattern that is the standard format\n",
    "    :param result: a list to append to all of the non-standard format responses for investigation\n",
    "    :return: None\n",
    "    \n",
    "    This function serves to find those values from a given row/column that do not fit a regex format supplied.\n",
    "    This can be used to ensure we are not missing some odd values during out data cleaning.\n",
    "    \n",
    "    TODO: Convert to a bigData Approach using Pyspark for scalability \n",
    "    \"\"\"\n",
    "    pattern = re.compile(patt)\n",
    "    if not pattern.match(row[met]):\n",
    "        result.append(row.Size)\n",
    "    return None\n",
    "\n",
    "\n",
    "mismatch = list()  # Initiate a list to contain results\n",
    "regexp = '(\\d+(?:\\.\\d+)?([Mk])$)'  # Looks for any int or float\n",
    "# Applies the pattern outlier function row by row\n",
    "df.apply(patt_out, axis=1, met='Size', patt=regexp, result = mismatch)\n",
    "mismatch = set(mismatch)  # Removes Duplicates by Converting to a Set\n",
    "mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**As you can see through our investigation of our data format for the Size column, everything follows the pattern:**\n",
    "- A Numeric Followed by 'M' for megabyte or 'k' for kilobyte\n",
    "- A Categorical Variable of 'Varies with device'\n",
    "\n",
    "Now to really get this feature into a category datatype we have to convert those with k or M to their corresponding \n",
    "number of bytes (shared unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_bytes(val:str) -> float:\n",
    "    \"\"\"\n",
    "    :param val: Values from a Series containing either a 'M' or 'k' to signify the unit of bytes\n",
    "    :return: converted to bytes value as a float\n",
    "    \n",
    "    This function serves to determine the byte units implied by the string and convert to raw kilobytes for comparison\n",
    "    \"\"\"\n",
    "    if val == 'Varies with device':\n",
    "        return 0.0\n",
    "    if val[-1] == 'M':\n",
    "        return float(val[:-1]) * 1000  # Multiply by 1000 to convert to kb\n",
    "    if val[-1] == 'k':\n",
    "        return float(val[:-1])\n",
    "\n",
    "sizes = df[df.Size != 'Varies with device'].Size  # Creates a series of all sizes we have data on\n",
    "sizes = sizes.apply(convert_bytes)  # Converts the sizes to the same unit, kilobytes\n",
    "\n",
    "sizes.describe()  # View General Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can see some basic descriptive statistics from this metric, we can start to gain insight on how we\n",
    "should categorize this variable.  Of interest we can see the average size is approx. 20,000 kb for our apps, however \n",
    "the variance in this metric is very high as well with a standard deviation of almost 22,000 kb.  Due to this I find it \n",
    "may be valuable to get a better look at the distribution of this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(sizes, shade=False, legend=None)  # Create a Subplot to get the Kernel Density Estimation Plot\n",
    "lines = ax.get_lines()[0]  # Grab the lin objects from the kde\n",
    "x, y = lines.get_data()  # convert line into x & y coords\n",
    "\n",
    "# Color fill the visual based on percentile - Default Interpolation\n",
    "ax.fill_between(x, y, where = x <= np.percentile(sizes, 25), color='r', alpha=0.5)\n",
    "ax.fill_between(x, y, where = (x > np.percentile(sizes, 25)) & \n",
    "                              (x < np.percentile(sizes, 75)), color='y', alpha=0.5)\n",
    "ax.fill_between(x, y, where = x >= np.percentile(sizes, 75), color='g', alpha=0.5)\n",
    "\n",
    "# Add vertical lines to signify the percentiles we shaded by\n",
    "ax.vlines(x = np.percentile(sizes, 25), ymin=0, ymax=np.interp(np.percentile(sizes, 25), x, y))\n",
    "ax.vlines(x = np.percentile(sizes, 75), ymin=0, ymax=np.interp(np.percentile(sizes, 75), x, y))\n",
    "ax.vlines(x = np.percentile(sizes, 50), ymin=0, ymax=np.interp(np.percentile(sizes, 50), x, y), color='r')\n",
    "\n",
    "#Add Data Labels for our lines to avoid confusion\n",
    "ax.text(np.percentile(sizes, 25), np.interp(np.percentile(sizes, 25), x, y), '25th Perc.')\n",
    "ax.text(np.percentile(sizes, 50), np.interp(np.percentile(sizes, 50), x, y), 'Median')\n",
    "ax.text(np.percentile(sizes, 75), np.interp(np.percentile(sizes, 75), x, y), '75th Perc.')\n",
    "\n",
    "ax.set_title('Google App Size Distribution')  # Add a title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see in the above Kernel Density Plot (used as an approximation of the PDF) the size data is heavily skewed to \n",
    "left and smaller apps.  This will mean that there is much more variation in the upper percentiles when it comes to App \n",
    "size.  There are two main ways people tend to lean when presented with this type of situation.  \n",
    "- That you have so many apps clustered together on the small side that we need additional categories on that side to \n",
    "see the true nuance.\n",
    "- Since there is so much variance (large spread) at the top percentiles we will want to have extra categories on that \n",
    "side of the distribution to account for different behavior.\n",
    "\n",
    "In cases like these I always find it beneficial to apply Fisher-Jenks algorithm first to help confirm the insight \n",
    "we've gained from our early analysis.  While this algorithm will find the natural breaking points for us in the data, \n",
    "we want to evaluate the distribution both with and without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Inner Quartile Range\n",
    "IQR = (np.percentile(sizes, 75) - np.percentile(sizes, 25))  # Establish the Inner Quartile Range\n",
    "upper_bound = np.percentile(sizes, 50) + (IQR * 1.5)\n",
    "lower_bound = np.percentile(sizes, 50) - (IQR * 1.5)\n",
    "\n",
    "sizes_trim = sizes[(sizes >= lower_bound) & (sizes <= upper_bound)]  # Trim off sizes outside our range\n",
    "\n",
    "# Print Results of each Percentile\n",
    "print(os.linesep, 'Outliers Not Removed')\n",
    "for i in range(0, 9):\n",
    "    print ((i + 1)*10, 'Percentile', np.percentile(sizes, (i +1)*10))\n",
    "\n",
    "# Establish the natural breaking points using Fisher-Jenks\n",
    "breaks = jenkspy.jenks_breaks(sizes, nb_class=5)\n",
    "print(os.linesep, 'FJ breaks', breaks)  # Print the Break Points\n",
    "\n",
    "# Print percentiles with outliers removed\n",
    "print(os.linesep, 'Outliers Removed')\n",
    "for i in range(0, 9):\n",
    "    print ((i + 1)*10, 'Percentile', np.percentile(sizes_trim, (i +1)*10))\n",
    "\n",
    "# Establish and print breaks using fisher-jenks algo with outliers removed \n",
    "breaks = jenkspy.jenks_breaks(sizes_trim, nb_class=5)\n",
    "print(os.linesep, 'FJ breaks', breaks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using the inner quartile range (*1.5) method to remove outliers has removed approx. 12% of our apps based on size. \n",
    "With a quick scan we can see that essentially they were all on the upper limits, however this seems like a large \n",
    "percentage of outliers.\n",
    "\n",
    "While it is unconventional, I think the appropriate action here is to keep outliers in but make a footnote of their \n",
    "inclusion.  In this case I will use the Fisher-Jenks results without outliers to build my categories, but then \n",
    "include the ranges when outliers are included, stating these are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['Size_Conv'] = df.Size.apply(convert_bytes)  # Convert bytes into a numeric column for investigation\n",
    "\n",
    "# Establish ranges \n",
    "criteria = df.Size_Conv.between(0, 8.5), df.Size_Conv.between(8.5, 6401), df.Size_Conv.between(6401, 13001), \\\n",
    "           df.Size_Conv.between(13001, 22001), df.Size_Conv.between(22001, 33001), df.Size_Conv.between(33001, 46001), \\\n",
    "           df.Size_Conv.between(46001, 69001), df.Size_Conv.between(69001, 100001)\n",
    "# Establish values associated with ranges\n",
    "values = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "#Apply the classification into it's own column\n",
    "df['Size_Class'] = np.select(criteria, values, 0)\n",
    "df['Size_Class'] = pd.Categorical(df.Size_Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we are down to the last two columns in our list for cleaning/converting.  First we will tackle 'Genre' which \n",
    "appears to be stored as a semi-colon separated list.  Our previous datatype search lets us know that it is stored \n",
    "as a string/text.  So, we will have to separate these genres and convert them into binary variables.  This is often \n",
    "referred to as 'One hot encoding\".  Thankfully Pandas has a somewhat simple process for doing so.  We will also take \n",
    "the necessary steps to encode our other categorical predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Separate the genres column on ';' and then use the get_dummies to turn into binary variables\n",
    "# Then we will rename each column as \"Genre_\" and whatever the genre names.  Last we concat this back to our\n",
    "# original dataframe so we have all the data together\n",
    "df = pd.concat([df, df.Genres.str.get_dummies(sep=';').rename(lambda c: 'Genre_' + c, axis='columns'),\n",
    "                df.Category.str.get_dummies(sep=',').rename(lambda c: 'Category_' + c, axis='columns'),\n",
    "                df['Content Rating'].str.get_dummies(sep=',').rename(lambda c: 'ContentRating_' + c, axis='columns')],\n",
    "               axis=1)\n",
    "\n",
    "\n",
    "# Check to ensure the genres were one hot encoded\n",
    "df[df.App == 'Basketball FRVR - Shoot the Hoop and Slam Dunk!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now these have been encoded as binary variables, we gain the opportunity to feature engineer if a game has one or\n",
    "multiple genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['multi_gen'] = df.loc[:, df.columns.str.startswith('Genre_')].sum(axis=1)  # Sum the Genres Column\n",
    "df.loc[df.multi_gen < 2, 'multi_gen'] = 0  # Map single genres to False\n",
    "df.loc[df.multi_gen >= 2, 'multi_gen'] = 1  # Map multi genres to true\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will also create a column for rounding ratings to it's closest 0.5 score as this may end up being useful for\n",
    "analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['rating_round'] = round(df.Rating * 2) / 2  # Rounds the rating to the closest 0.5x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Converting Last Updated to a date is fairly simple as long as you know the date format your data is currently in.\n",
    "In this case we want to convert it to a datetime variable using to_datetime and the format = %d-%b-%y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['Last Updated'] = pd.to_datetime(df['Last Updated'], format='%d-%b-%y')  # Convert text to Datetime\n",
    "df['Last Updated'].head()  # Verify that the data has been converted correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***At this point our Google Play Market data has been cleaned/converted and stands ready for analysis***\n",
    "\n",
    "We will export this to our RawData folder in case we need to reference it outside of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(r'RawData/google_cleaned.csv', index=False)  # Export df without index to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building Labels to Define Success\n",
    "If we are are going to complete any sort of feature selection or basic supervised learning \n",
    "action, we need to have labels to train to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Installs', data=df)  # Basic Count Plot of our Install Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the installs variable may need to be cleaned up.  Since this is a categorical variable \n",
    "based of numerics, they have a natural order. \n",
    "\n",
    "Since the data does not seem to be standardized, we may need to create a dictionary to save the correct order from \n",
    "checking the original data.  We know there are 21 categories, so it is not too hard to order.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Ordered list of installs\n",
    "inst = ['0','0+','1+','5+','10+','50+','100+','500+','1,000+','5,000+','10,000+','50,000+','100,000+','500,000+',\n",
    "        '1,000,000+','5,000,000+','10,000,000+','50,000,000+','100,000,000+','500,000,000+','1,000,000,000+']\n",
    "\n",
    "installs = {}  # Init a Dict to hold order and count\n",
    "for i, cat in enumerate(inst): # Loop through list\n",
    "    installs[cat] = [i, 0]  # establish the keys and values (order) and an empty count\n",
    "    \n",
    "installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have a dictionary of install categories and it is ordered by size.  We will now add a second value to each key \n",
    "with their counts from the dataset.  This should serve as a better visual for us to make our labels from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "for i in df.Installs:\n",
    "    installs[i][1] += 1  # for each key we come across increment the count value by one\n",
    "    \n",
    "installs  # Preview installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can see the distribution of our install groups, too keep this simple we should maybe define our labels to \n",
    "be based off of a certain percentile of responses.  We know we have approx. 8,800 responses, so we can easily break \n",
    "this distribution into Quartiles (Approx. 2,200 responses per quartile).  We are okay with rough estimates here as \n",
    "we are just investigating the current market.  We need some way to define a 'true' label, so we start with basic and \n",
    "see our results. \n",
    "\n",
    "1,000+ & Below: 2249 Responses (25.4 perc)\n",
    "5,000+ - 100,000+: 2751 Responses (56.4 perc)\n",
    "500,000+ - 5,000,000+: 2495 Responses (84.6 perc)\n",
    "10,000,000+ - 1,000,000,000+: 1367 Responses (100 perc)\n",
    "\n",
    "As you can see this does not work out well, but we can now have a starting point at least.  If we start the label of \n",
    "success at anything 10,000,000+ or above, we now know we are grabbing the top 43.6% of apps as far as installs go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "succ_group = [k for k, v in installs.items() if v[0]>=13]  # Create binary label for any install category with index 16+\n",
    "\n",
    "df['label'] = df.Installs.isin(succ_group)  # Attach labels to the dataframe\n",
    "\n",
    "# df.to_csv(r'RawData/google_labeled.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Analyzing our Data\n",
    "Now, lets look at the labels for success that we defined and see if we can gain any insight into what type of apps meet \n",
    "our criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp = df[['Category','label']].groupby('Category').agg(['mean', 'count'])  # Group and Aggregate on our label\n",
    "temp.sort_values(('label', 'mean'), ascending=False)[:5]  # Sort and show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see from a basic average of our labels the top 5 app Categories are:\n",
    "- Photography\n",
    "- Entertainment\n",
    "- Game\n",
    "- Education\n",
    "- Weather\n",
    "\n",
    "This is showing that our top categories have a \"success\" rate of approx. 67 - 87%\n",
    "It is also to note the sizes of each group.  While they are not our largest categories I believe they \n",
    "are of sufficient size for this analysis.\n",
    "\n",
    "Now we will move onto doing this same process for some of our other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp = df[['Content Rating','label']].groupby('Content Rating').agg(['mean', 'count'])  # Group and aggregate on label\n",
    "temp.sort_values(('label', 'mean'), ascending=False)[:5]  # Sort and show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Of interest, it seems that games rated for young kids (teens) appear to have the highest chance of success.\n",
    "Though due to the unbalanced sample sizes it may only be fair to say that having a rating outside of \n",
    "'Everyone' is the best practice.  Due to this I would suggest some level of mature content appropriate for \n",
    "young teens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp = df[['Size_Class','label']].groupby('Size_Class').agg(['mean', 'count'])  # Group and aggregate on our label\n",
    "temp.sort_values(('label', 'mean'), ascending=False)  # Sort & show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see clearly there are two things going on here. The smallest Apps (Category 1) appear to be very \n",
    "successful at being installed.  It seems likely that these are utility apps, meant to accomplish one small \n",
    "goal.  After that there is a clear pattern that the larger Apps appear to be the most successful.\n",
    "\n",
    "So my take away here is that there are probably two types of successful app strategies, create a utility app \n",
    "that provides small amounts of functionality OR build an large app that provides a very large experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp = df[['rating_round','label']].groupby('rating_round').agg(['mean', 'count'])  # Group & Agg on our label\n",
    "temp.sort_values(('label', 'mean'), ascending=False)  # Sort and show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Interestingly we can see that ratings seems to have a strong correlation with our success criteria except\n",
    "the 5.0 ratings appear to be throwing it off.  It seems fair to say that the only apps that receive such high\n",
    "ratings are those that do not have a lot of opinions (low reviews and installs).\n",
    "\n",
    "Moving on to dig into the genre category is more complicated as some apps can have multiple genres.  We\n",
    "already handled this using one hot encoding, but not lets look at the success rates of apps depending on\n",
    "genres.  We will be removing any genre that doesn't have at least 100 apps to limit the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "genres = df.loc[:, df.columns.str.startswith('Genre_')].columns  # Genre Column list\n",
    "results = {}  # Init dict\n",
    "for i, col in enumerate(genres):  # Loop through columns\n",
    "    temp = df[df[col] == 1]  # init temp df containing only that genre's apps\n",
    "    met = temp.label.mean()  # Establish the mean of the success labels\n",
    "    cnt = len(temp)  # establish how many apps fit that genre\n",
    "    if cnt < 100:  # Filter our results to only contain 100 or more apps\n",
    "        continue\n",
    "    else:\n",
    "        results[col] = [met, cnt]  # create a key and append values\n",
    "\n",
    "# Sort the dict by it's first item in reverse order\n",
    "results = {k: v for k, v in sorted(results.items(), key=lambda item: item[1],reverse=True)}\n",
    "results  # Preview results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now see that certain genres appear to be very successful in garnering installs at a high rate:\n",
    "- Racing\n",
    "- Arcade\n",
    "- Action\n",
    "- Photography\n",
    "- Casual\n",
    "\n",
    "## Initial Findings\n",
    "Through our cleaning, feature engineering and basic analysis we can clearly say that we have the ability to\n",
    "narrow down the scope to what types of apps would be most successful.  For example, we know that a **Casual Racing\n",
    "Game App that is large (offer a very robust experience) and has good early reviews should be quite successful**\n",
    "\n",
    "However, our goal isn't to just make one suggestion to our designers but to be able to make numerous or even\n",
    "take ideas and provide feedback on whether we believe it will garner a lot of installs or not.  To do this\n",
    "we will take our labels of success and our features and see if we can establish a predictive model that could\n",
    "take basic features of the App including reviews from a test group and determine if the app is likely to be\n",
    "installed at a large scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predictive Model\n",
    "\n",
    "Now it is time to trim off the unnecessary columns from our data set so that we can convert our df into valid arrays \n",
    "for our model.  We will be removing both size_cat and rating_round as they are likely to be\n",
    "collinear with size_class & ratings,  This will lower the sparsity of the data and should result\n",
    "in a more efficient model.\n",
    "\n",
    "It is also important to determine what we would consider to be successful for our model.  In this situation\n",
    "it is always good to know what type of model we would like:\n",
    "- Sensitive\n",
    "- Specific\n",
    "\n",
    "This will let us know how to tune our model.  In this case it seems to be more common to be looking\n",
    "for a specific model.  This means that we will not often make False predictions of success but\n",
    "this may come at the cost of missing some successful apps.  This ensures that we are very confident\n",
    "of our prediction for success, but not necessarily confident in our prediction of unsuccessful.\n",
    "\n",
    "In this case it makes sense that we want to be correct when we predict success at least 85% of the time,\n",
    "but we also want to make sure we are not missing too many successful app ideas, meaning we will\n",
    "still want a decent sensitivity (maybe 50%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "remove = ['App', 'Category', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated',\n",
    "          'Current Ver', 'Android Ver', 'Reviews', 'rating_round', 'Size_Class']  # Establish list of unnecessary cols\n",
    "\n",
    "\n",
    "df.drop(remove, axis=1, inplace=True)  # Drop Col list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this case I usually find it valuable to start with a simple model such as a Decision tree to\n",
    "evaluate the likelihood of success.  From there we can go more advance such as a random forest, but\n",
    "I want to ensure to start as a decision tree also for its ability to return feature importance.\n",
    "\n",
    "Even if we cannot build a successful model, the feature importance can prove as useful insight for\n",
    "our app developers even if this is too complicated of a problem for our current dataset.\n",
    "\n",
    "### Decision Tree\n",
    "We will first separate our labels from our DF so that we can split the data by the 80/20 rule.\n",
    "This will provide us with both datasets for training the model and testing it's performance, but\n",
    "also the labels to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "RSEED=500  # Establish a seed\n",
    "\n",
    "# Create our Labels\n",
    "labels = np.array(df.pop('label'))\n",
    "\n",
    "# Split our data by 80/20 rule\n",
    "train, test, train_labels, test_labels = train_test_split(df, labels, stratify=labels, test_size=0.20, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In our care the only issue we may have with our data is missing values in the rating field.  There\n",
    "are many ways to handle this but for this project I think it makes most sense to replace those missing ratings\n",
    "with the average results for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Impute ratings column for missing values\n",
    "train.Rating = train.Rating.fillna(train.Rating.mean())\n",
    "test.Rating = test.Rating.fillna(test.Rating.mean())\n",
    "\n",
    "# Features for feature importances\n",
    "features = list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preview the size of our train set\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preview the size of our test set\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have prepared our data, it is time to train the tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train the tree\n",
    "tree = DecisionTreeClassifier(random_state=RSEED)  # Establish a tree using our seed\n",
    "\n",
    "tree.fit(train, train_labels)  # Train the tree\n",
    "\n",
    "# Basic Tree Information\n",
    "print(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will now make predictions and grab the predicted probabilities for both sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Establish the probabilities of fitting into the class\n",
    "train_probs = tree.predict_proba(train)[:, 1]\n",
    "probs = tree.predict_proba(test)[:, 1]\n",
    "\n",
    "# Make the predictions\n",
    "train_predictions = tree.predict(train)\n",
    "predictions = tree.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can review our initial accuracy scores and evaluate our model's performance.  We will create\n",
    "a function to evaluate our models performance because we will have to refer back to it several times\n",
    "throughout this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Compare machine learning model to baseline performance.\n",
    "Computes statistics and shows ROC curve.\"\"\"\n",
    "def evaluate_model(predictions, probs, train_predictions, train_probs):\n",
    "    \"\"\"\n",
    "    :param predictions: Test Predicitions from our model\n",
    "    :param probs: Test probabilities from our model\n",
    "    :param train_predictions: Train Predicitions from our model\n",
    "    :param train_probs: Train probabilities from our model\n",
    "    :return: None\n",
    "\n",
    "    This will take our test results from our model and compare them to baseline and training\n",
    "    results.  it will calculate precision and recall (specificity and Sensitivity) as well\n",
    "    as display the ROC curve\n",
    "    \"\"\"\n",
    "    baseline = {}\n",
    "    \n",
    "    baseline['recall'] = recall_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['precision'] = precision_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['recall'] = recall_score(test_labels, predictions)\n",
    "    results['precision'] = precision_score(test_labels, predictions)\n",
    "    results['roc'] = roc_auc_score(test_labels, probs)\n",
    "    \n",
    "    train_results = {}\n",
    "    train_results['recall'] = recall_score(train_labels, train_predictions)\n",
    "    train_results['precision'] = precision_score(train_labels, train_predictions)\n",
    "    train_results['roc'] = roc_auc_score(train_labels, train_probs)\n",
    "    \n",
    "    for metric in ['recall', 'precision', 'roc']:\n",
    "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')\n",
    "    \n",
    "    # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(test_labels, probs)\n",
    "    \n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend()\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(predictions, probs, train_predictions, train_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have a baseline of accuracy for our model it is often useful to visualize the confusion\n",
    "matrix for review of our findings.  We will create a function to do this as we will refer back to\n",
    "this several times throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    :param cm: Our Confusion Matric\n",
    "    :param classes: Our Classes we are prediciting\n",
    "    :param normalize: Represent as a range from 0 - 1\n",
    "    :param title: Title of the plot\n",
    "    :param cmap: Color Map to plot with\n",
    "    :return: Returns the TP, FP, TN, FN results from our Confuision Matrix\n",
    "     This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    if normalize:  # Establish whether to normalize or not\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # print(cm)\n",
    "\n",
    "    # plt confusion matrix\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    # Establishes format\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)\n",
    "    \n",
    "    return cm[1, 1], cm[0,0], cm[0, 1], cm[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Init a Confusion Matrix from our test results\n",
    "cm = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "# Display the confusion Matrix and return the Quadrant results\n",
    "TP, TN, FP, FN = plot_confusion_matrix(cm, classes = ['Unsuccessful App', 'Successful App'],\n",
    "                      title = 'App Confusion Matrix')\n",
    "\n",
    "# Print Classification Error Score\n",
    "print('Overall, how often the model is incorrect.',os.linesep, 'Classification Error: ', ((FP + FN)/float(TP + TN + FP + FN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we understand our model's performance we can start looking into which features drove\n",
    "our models decisions the most.  To do this we need to compute the feature importance for each\n",
    "feature.  Thankfully decision trees have this feature built into their library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a feature importance dataframe for review\n",
    "fi = pd.DataFrame({'feature': features,\n",
    "                   'importance': tree.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "fi.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#TODO: Now we need to explain our initial results and describe how they may be promising\n",
    "then we need to explain our ensemble model to improve performance, etc..\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               random_state=RSEED, \n",
    "                               max_features = 'sqrt',\n",
    "                               n_jobs=-1, verbose = 1)\n",
    "\n",
    "# Fit on training data\n",
    "model.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_nodes = []\n",
    "max_depths = []\n",
    "\n",
    "for ind_tree in model.estimators_:\n",
    "    n_nodes.append(ind_tree.tree_.node_count)\n",
    "    max_depths.append(ind_tree.tree_.max_depth)\n",
    "    \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_rf_predictions = model.predict(train)\n",
    "train_rf_probs = model.predict_proba(train)[:, 1]\n",
    "\n",
    "rf_predictions = model.predict(test)\n",
    "rf_probs = model.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(test_labels, rf_predictions)\n",
    "TP, TN, FP, FN = plot_confusion_matrix(cm, classes = ['Unsuccessful App', 'Successful App'],\n",
    "                      title = 'App Confusion Matrix')\n",
    "print('Overall, how often the model is incorrect.',os.linesep, 'Classification Error: ', ((FP + FN)/float(TP + TN + FP + FN)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fi_model = pd.DataFrame({'feature': features,\n",
    "                   'importance': model.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "fi_model.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': np.linspace(10, 200).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = RandomForestClassifier(random_state = RSEED)\n",
    "\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = 'roc_auc', cv = 3, \n",
    "                        n_iter = 10, verbose = 1, random_state=RSEED)\n",
    "\n",
    "# Fit \n",
    "rs.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rs.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_rf_predictions = best_model.predict(train)\n",
    "train_rf_probs = best_model.predict_proba(train)[:, 1]\n",
    "\n",
    "rf_predictions = best_model.predict(test)\n",
    "rf_probs = best_model.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_nodes = []\n",
    "max_depths = []\n",
    "\n",
    "for ind_tree in best_model.estimators_:\n",
    "    n_nodes.append(ind_tree.tree_.node_count)\n",
    "    max_depths.append(ind_tree.tree_.max_depth)\n",
    "    \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, rf_predictions)\n",
    "TP, TN, FP, FN = plot_confusion_matrix(cm, classes = ['Unsuccessful App', 'Successful App'],\n",
    "                      title = 'App Confusion Matrix')\n",
    "print('Overall, how often the model is incorrect.',os.linesep, 'Classification Error: ', ((FP + FN)/float(TP + TN + FP + FN)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(test_labels, rf_probs)\n",
    "\n",
    "def evaluate_thresh(threshold):\n",
    "    print('Recall: ', round(tpr[thresholds > threshold][-1], 3))\n",
    "    print('Precision: ', round(1 - fpr[thresholds > threshold][-1], 3))\n",
    "    print(os.linesep)\n",
    "\n",
    "for i in range(20, 80, 5):\n",
    "    print('Threshold: ' + str(round(i * 0.01, 2)))\n",
    "    evaluate_thresh(i*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Goal of 85% Precision\n",
    "new_threshold = 0.6\n",
    "\n",
    "y_pred_class = binarize(rf_probs.reshape(-1, 1), threshold=new_threshold)\n",
    "cm = confusion_matrix(test_labels, y_pred_class)\n",
    "TP, TN, FP, FN = plot_confusion_matrix(cm, classes = ['Unsuccessful App', 'Successful App'],\n",
    "                      title = 'App Confusion Matrix')\n",
    "print('Overall, how often the model is incorrect.', os.linesep, 'Classification Error: ', ((FP + FN)/float(TP + TN + FP + FN)))\n",
    "print('Recall: ', TP / (FN + TP))\n",
    "print('Precision: ', TN / (TN + FP))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}